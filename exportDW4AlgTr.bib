@article{Al-RahmanWebETL2023,
   abstract = {Owing to the recent increased size and complexity of data in addition to management issues, data storage requires extensive attention so that it can be employed in realistic applications. Hence, the requirement for designing and implementing a data warehouse system has become an urgent necessity. Data extraction, transformation and loading (ETL) is a vital part of the data warehouse architecture. This study designs a data warehouse application that contains a web ETL process that divides the work between the input device and server. This system is proposed to solve the lack of work partitioning between the input device and server. The designed system can be used in many branches and disciplines because of its high performance in adding data, analyzing data by using a web server and building queries. Analysis of the results proves that the designed system is fast in cleaning and transferring data between the remote parts of the system connected to the internet. ETL without missing any data consumes 0.00582 seconds. This is an open access article under the CC BY-SA license.},
   author = {Seddiq Q Abd Al-Rahman and Ekram H Hasan and Ali Makki Sagheer},
   doi = {10.11591/ijai.v12.i2.pp765-775},
   issn = {2252-8938},
   issue = {2},
   journal = {IAES International Journal of Artificial Intelligence (IJ-AI},
   keywords = {Data warehouse Dirty data Fact table Interim table Web (extract,load),transform},
   pages = {765-775},
   title = {Design and implementation of the web (extract, transform, load) process in data warehouse application},
   volume = {12},
   url = {http://ijai.iaescore.com},
   year = {2023},
}
@article{SeenivasanETL2023,
   abstract = {This article provides an overview of the key principles and techniques for effectively extracting, transforming, and loading data from various sources into a target system. It covers topics such as data quality checks, testing, performance optimization, and security. The article aims to provide readers with a comprehensive understanding of the best practices for ETL to improve the efficiency, accuracy, and reliability of their data pipeline and provides actionable advice for implementing ETL processes successfully.},
   author = {Dhamotharan Seenivasan},
   doi = {10.14445/22312803/IJCTT-V71I1P106},
   journal = {International Journal of Computer Trends and Technology},
   keywords = {Data warehouse,ETL jobs,ETL optimization,ETL performance,Extract Transform and Load (ETL)},
   pages = {40-44},
   title = {ETL (Extract, Transform, Load) Best Practices},
   volume = {71},
   url = {https://doi.org/10.14445/22312803/IJCTT-V71I1P106},
   year = {2023},
}
@generic{Postgres2022,
   author = {The PostgreSQL Global Development Group},
   editor = {The PostgreSQL Global Development Group},
   pages = {445-456},
   title = {Documentation PostgreSQL 10.20},
   year = {2022},
}
@article{Rahim2021,
   abstract = {Advanced householder profiling using digital water metering data analytics has been acknowledged as a core strategy for promoting water conservation because of its ability to provide near real-time feedback to customers and instil long-term conservation behaviours. Customer profiling based on household water consumption data collected through digital water meters helps to identify the water consumption patterns and habits of customers. This study employed advanced customer profiling techniques adapted from the machine learning research domain to analyse high-resolution data collected from residential digital water meters. Data analytics techniques were applied on already disaggregated end-use water consumption data (e.g., shower and taps) for creating in-depth customer profiling at various intervals (e.g., 15, 30, and 60 min). The developed user profiling approach has some learning functionality as it can ascertain and accommodate changing behaviours of residential customers. The developed advanced user profiling technique was shown to be beneficial since it identified residential customer behaviours that were previously unseen. Furthermore, the technique can identify and address novel changes in behaviours, which is an important feature for promoting and sustaining long-term water conservation behaviours. The research has implications for researchers in data analytics and water demand management, and also for practitioners and government policy advisors seeking to conserve valuable potable-water resources.},
   author = {Md Shamsur Rahim and Khoi Anh Nguyen and Rodney Anthony Stewart and Damien Giurco and Michael Blumenstein},
   doi = {https://doi.org/10.1016/j.jenvman.2021.112377},
   issn = {0301-4797},
   journal = {Journal of Environmental Management},
   keywords = {Behaviour change,Digital water meter,Recommender system,User profiling,Water conservation,Water consumption data},
   pages = {112377},
   title = {Advanced household profiling using digital water meters},
   volume = {288},
   url = {https://www.sciencedirect.com/science/article/pii/S0301479721004394},
   year = {2021},
}
@article{Qu2021,
   author = {Weiping Qu},
   title = {On-Demand ETL for Real-Time Analytics},
   year = {2021},
}
@article{Chauhan2021,
   author = {Preeti Chauhan and Mohit Sood},
   issue = {04},
   journal = {Computer},
   pages = {59-65},
   publisher = {IEEE Computer Society},
   title = {Big Data: Present and Future},
   volume = {54},
   year = {2021},
}
@article{Aziz2021,
   author = {O Aziz and T Anees and E Mehmood},
   doi = {10.1109/ACCESS.2021.3064202},
   issn = {2169-3536 VO - 9},
   journal = {IEEE Access},
   pages = {41261-41274},
   title = {An Efficient Data Access Approach With Queue and Stack in Optimized Hybrid Join},
   volume = {9},
   year = {2021},
}
@inproceedings{Karthik2021,
   abstract = {This paper is to provide a solution for building Natural Language Interface to Database (NLIDB) using Metadata configuration approach. Nowadays, using systems like Natural Language Interface to Databases aim to provide an effortless way to access enterprise’s database system by ensuring the user need not learn any formal technical languages. Traditionally, NLIDB systems were considered as a dead-end to researchers who used to face immense challenges in building such systems which can easily translate human language to computational linguistics. And the major challenges in Natural language Query processing are endless, including the problems related to inherent ambiguity which a natural language possesses such as interpreting the query correctly, removal of various ambiguity, and mapping to the appropriate context. In this paper, we’ll propose our methodology for NLIDB analytics allowing a computer user having non-technical background to access enterprise database easily. This analysis provides a solid approach to solve relational database queries using NLP techniques through metadata configurations and thereby addressing all the challenges mentioned above.},
   author = {M. N. Karthik and Garima Makkar},
   doi = {10.1007/978-981-15-5616-6_17},
   isbn = {9789811556159},
   issn = {21945365},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {Database,Entities,NLIDB,NLP,Natural language,SQL},
   pages = {231-241},
   publisher = {Springer},
   title = {NLIDB Systems for Enterprise Databases: A Metadata Based Approach},
   volume = {1174},
   year = {2021},
}
@inproceedings{Vathsala2021,
   abstract = {Human Computer interaction has been moving towards Natural language in the modern age. SQL (Structured Query Language) is the chief database query language used today. There are many flavors of SQL but all of them have the same basic underlying structure. This paper attempts to use the Natural Language inputs to query the databases, which is achieved by translating the natural language (which in our case is English) input into the SQL (specific to MySQL database) query language. Here we use a semi-supervised learning with Memory augmented policy optimization approach to solve this problem. This method uses the context of the natural language questions through database schema, and hence its not just generation of SQL code. We have used the WikiSQL dataset for all our experiments. The proposed method gives a 2.3% higher accuracy than the state of the art semi-supervised method on an average.},
   author = {H Vathsala and Shashidhar G Koolagudi},
   city = {Singapore},
   editor = {Deepak Garg and Kit Wong and Jagannathan Sarangapani and Suneet Kumar Gupta},
   isbn = {978-981-16-0401-0},
   journal = {Advanced Computing},
   pages = {288-299},
   publisher = {Springer Singapore},
   title = {NLP2SQL Using Semi-supervised Learning},
   year = {2021},
}
@inproceedings{Wei2021,
   author = {Ziyun Wei and Immanuel Trummer and Connor Anderson},
   journal = {Proceedings of the 2021 International Conference on Management of Data},
   pages = {2798-2802},
   title = {Demonstrating Robust Voice Querying with MUVE: Optimally Visualizing Results of Phonetically Similar Queries},
   year = {2021},
}
@inproceedings{Trummer2021,
   author = {Immanuel Trummer and Connor Anderson},
   institution = {IEEE},
   journal = {2021 IEEE 37th International Conference on Data Engineering (ICDE)},
   pages = {1715-1726},
   title = {Optimally Summarizing Data by Small Fact Sets for Concise Answers to Voice Queries},
   year = {2021},
}
@article{Mushtaq2021,
   author = {Muhammad Hamzah Mushtaq},
   journal = {arXiv preprint arXiv:2102.11047},
   title = {Semantic Parsing to Manipulate Relational Database For a Management System},
   year = {2021},
}
@article{Sulir2021,
   author = {Matúš Sul\'\ir and Jaroslav Porubän},
   issue = {1},
   journal = {Open Computer Science},
   pages = {135-145},
   publisher = {De Gruyter},
   title = {Natural mapping between voice commands and APIs},
   volume = {11},
   year = {2021},
}
@article{Edinat2021,
   author = {Afaf Khaleel Edinat and Fatima Omar Haimour and others},
   title = {Data Vocalization Enhancing Voice Output of Relational Data Base},
   year = {2021},
}
@inproceedings{Kale2021,
   author = {Swati Kale and Vinay Gahse and Piyush Mahatkar and Ruhit Ratnaparkhi and Nakul Dhawale and Tanay Totade},
   issue = {1},
   institution = {IOP Publishing},
   journal = {Journal of Physics: Conference Series},
   pages = {12146},
   title = {An intelligent system for transforming natural language queries into SQL and its execution},
   volume = {1913},
   url = {https://iopscience.iop.org/article/10.1088/1742-6596/1913/1/012146/pdf},
   year = {2021},
}
@inproceedings{Brunner2021,
   author = {Ursin Brunner and Kurt Stockinger},
   institution = {IEEE},
   journal = {2021 IEEE 37th International Conference on Data Engineering (ICDE)},
   pages = {2177-2182},
   title = {Valuenet: A natural language-to-sql system that learns from database information},
   year = {2021},
}
@article{OHalloran2021,
   author = {Kay L O'Halloran and Gautam Pal and Minhao Jin},
   journal = {Discourse, Context \& Media},
   pages = {100467},
   publisher = {Elsevier},
   title = {Multimodal approach to analysing big social and news media data},
   volume = {40},
   year = {2021},
}
@article{Maulud2021,
   author = {Dastan Hussen Maulud and Siddeeq Y Ameen and Naaman Omar and Shakir Fattah Kak and Zryan Najat Rashid and Hajar Maseeh Yasin and Ibrahim Mahmood Ibrahim and Azar Abid Salih and Nareen O M Salim and Dindar Mikaeel Ahmed},
   journal = {Asian Journal of Research in Computer Science},
   pages = {1-17},
   title = {Review on Natural Language Processing Based on Different Techniques},
   year = {2021},
}
@inproceedings{Mahjadi2021,
   author = {Khadija Majhadi and Mustapha Machkour},
   institution = {EDP Sciences},
   journal = {E3S Web of Conferences},
   pages = {1039},
   title = {The history and recent advances of Natural Language Interfaces for Databases Querying},
   volume = {229},
   url = {https://www.e3s-conferences.org/articles/e3sconf/pdf/2021/05/e3sconf_iccsre2021_01039.pdf},
   year = {2021},
}
@article{Maulud2021,
   author = {Dastan Hussen Maulud and Subhi R M Zeebaree and Karwan Jacksi and Mohammed A Mohammed Sadeeq and Karzan Hussein Sharif},
   issue = {2},
   journal = {Qubahan Academic Journal},
   pages = {21-28},
   title = {State of art for semantic analysis of natural language processing},
   volume = {1},
   url = {https://journal.qubahan.com/index.php/qaj/article/download/44/29},
   year = {2021},
}
@generic{Lembke2021,
   author = {Svan Lembke and Youry Khmelevsky and Lee Cartier},
   title = {Developing industry-wide information management capabilities: A case study from British Columbia's tree fruit industry},
   url = {https://arxiv.org/pdf/2102.05768.pdf},
   year = {2021},
}
@inproceedings{Narechania2021,
   abstract = {Designing natural language interfaces for querying databases remains an important goal pursued by researchers in natural language processing, databases, and HCI. These systems receive natural language as input, translate it into a formal database query, and execute the query to compute a result. Because the responses from these systems are not always correct, it is important to provide people with mechanisms to assess the correctness of the generated query and computed result. However, this assessment can be challenging for people who lack expertise in query languages. We present Debug-It-Yourself (DIY), an interactive technique that enables users to assess the responses from a state-of-the-art natural language to SQL (NL2SQL) system for correctness and, if possible, fix errors. DIY provides users with a sandbox where they can interact with (1) the mappings between the question and the generated query, (2) a small-but-relevant subset of the underlying database, and (3) a multi-modal explanation of the generated query. End-users can then employ a back-of-the-envelope calculation debugging strategy to evaluate the system’s response. Through an exploratory study with 12 users, we investigate how DIY helps users assess the correctness of the system’s answers and detect & fix errors. Our observations reveal the benefits of DIY while providing insights about end-user debugging strategies and underscore opportunities for further improving the user experience.},
   author = {Arpit Narechania and Adam Fourney and Bongshin Lee and Gonzalo Ramos},
   city = {New York, NY, USA},
   doi = {10.1145/3397481.3450667},
   isbn = {9781450380171},
   journal = {26th International Conference on Intelligent User Interfaces},
   keywords = {database systems,human computer interaction,natural language interface},
   pages = {597–607},
   publisher = {Association for Computing Machinery},
   title = {DIY: Assessing the Correctness of Natural Language to SQL Systems},
   url = {https://doi-org.ezproxy.okanagan.bc.ca/10.1145/3397481.3450667},
   year = {2021},
}
@generic{Martins2021,
   abstract = {Relational databases are getting bigger and more complex. Also, current Database Management Systems (DBMSs) need to respond efficiently to operations on their data. In this context, database optimization is evident as a process of refining database systems, aiming to improve their throughput and performance. This paper evaluates and compares the performance of Oracle and PostgreSQL database systems with the TPC-H benchmark, following a strategy of adding column-based indexes to optimize query execution. Ten TPC-H queries are performed on tables without any restrictions, with primary and foreign keys and with index constraints. The performance in each set of executions is analyzed. The results allow inferring a positive impact when using constraints with a significant speedup as well as better throughput. Oracle has shown stability and robustness for queries, with best results in scenarios with poor performance conditions. However, PostgreSQL showed shorter execution times after the optimizations made and proved to be more sensitive. Global performance results show that Oracle can improve 7% performance with indexes and PostgreSQL 91%. When comparing the results of Oracle with PostgreSQL, no indexes, Oracle/PostgreSQL is 64% faster, and with indexes, PostgreSQL/Oracle is 75% faster.},
   author = {Pedro Martins and Paulo Tomé and Cristina Wanzeller and Filipe Sá and Maryam Abbasi},
   city = {Cham},
   doi = {10.1007/978-3-030-72651-5_46},
   isbn = {2194-5357},
   journal = {Trends and Applications in Information Systems and Technologies},
   keywords = {DBMS,DSS,Database optimization,Oracle,PostgreSQL,TPC-H},
   pages = {481-490},
   publisher = {Springer International Publishing},
   title = {Comparing Oracle and PostgreSQL, Performance and Optimization},
   url = {https://go.exlibris.link/s8crchW0},
   year = {2021},
}
@magazine_article{IBM2021,
   author = {Tanmay Sinha},
   journal = {IBM Blog},
   month = {3},
   title = {OLAP vs. OLTP: What's the Difference?},
   url = {https://www.ibm.com/cloud/blog/olap-vs-oltp},
   year = {2021},
}
@inproceedings{Suleykin2020,
   abstract = {Digital transformation of a railway system based on big data technologies relies on integrating large volumes of streaming data into digitally enabled enterprise systems to form a comprehensive and efficient intelligent transportation system. Data requirements of the smart railway transportation involve a large number of unstructured data and semi-structured data including railway KPI data. Traditional ETL technology cannot cope with fast growing demands of processing large volumes of real-time data collected from heterogeneous sources both inside the system and in the environment. According to the characteristics of the railway KPI data, this paper proposes the designs of an automated ETL system with higher versatility and efficiency of data processing. To reach the goals, we optimize the workflow of the ETL using a proprietary designed metadata management framework. Making ETL suitable for big data-driven railway transportation environment, requires redesigning the ETL processing rules by using metadata model and then optimizing the extracting, transforming and loading processes of the ETL system. Our experimental results with actual railway KPI data show that the proposed metadata supported automated ETL system can effectively serve the railway KPI data processing using open source distributed big data technologies. The proposed metadata framework proved to be efficient in processing complex data structures and large data capacity of big data.},
   author = {A Suleykin and P Panfilov},
   doi = {10.1109/BigData50022.2020.9378367},
   isbn = {VO  -},
   journal = {2020 IEEE International Conference on Big Data (Big Data)},
   pages = {2433-2442},
   title = {Metadata-Driven Industrial-Grade ETL System},
   year = {2020},
}
@article{Li2020,
   abstract = {The challenge of natural language processing is from natural language to logical form (SQL). In this article, we present an fuzzy semantic to structured query language (F-SemtoSql) neural approach that is a fuzzy decision semantic deep network query model based on demand aggregation. It aims to address the problem of the complex and cross-domain text-to-SQL generation task. The corpus is trained as the input word vector of the model with LSTM and Word2Vec embedding technology. Combined with the dependency graph method, the problem of SQL statement generation is converted to slot filling. Complex tasks are divided into four levels via F-SemtoSql and constructed by the need of aggregation. At the same time, to avoid the order problem in the traditional model effectively, we have adopted the attention mechanism and used a fuzzy decision mechanism to improve the model decision. On the challenging text-to-SQL benchmark Spider and the other three datasets, F-SemtoSql achieves faster convergence and occupies the first position.},
   author = {Qing Li and Lili Li and Qi Li and Jiang Zhong},
   doi = {10.1109/TII.2019.2952929},
   issn = {19410050},
   issue = {4},
   journal = {IEEE Transactions on Industrial Informatics},
   keywords = {Fuzzy decision,fuzzy semantic deep network,natural language processing (NLP),text-to-SQL},
   month = {4},
   pages = {2542-2550},
   publisher = {IEEE Computer Society},
   title = {A Comprehensive Exploration on Spider with Fuzzy Decision Text-to-SQL Model},
   volume = {16},
   year = {2020},
}
@article{Rubin2020,
   author = {Ohad Rubin and Jonathan Berant},
   journal = {CoRR},
   title = {SmBoP: Semi-autoregressive Bottom-up Semantic Parsing},
   volume = {abs/2010.1},
   url = {https://arxiv.org/abs/2010.12412},
   year = {2020},
}
@article{Ranti2020,
   abstract = {Nowadays, every company knows that when making a decision that has a potential in affecting their assets, an accurately processed report is necessary in order to support the reasoning behind their decision. Generating a report for stakeholders quickly and accurately is highly required in assisting them making a data-driven decision. By developing a data warehouse, it is possible for a company to do a data-driven decision making to appeal to their customer segments. This paper proposes a data warehouse model design to analyse the sales data contained in the database. The method that was implemented for this particular data warehouse development is the nine-step methodology designed by Kimball. The results are then presented in pdf form and an interactive dashboard.},
   author = {Kiefer Stefano Ranti and Deyanara Tuapattinaya and Calvin Chang and Abba Suganda Girsang},
   doi = {10.1088/1742-6596/1477/3/032013},
   journal = {Journal of Physics: Conference Series},
   month = {3},
   pages = {32013},
   publisher = {\{IOP\} Publishing},
   title = {Data warehouse for analysing music sales on a digital media store},
   volume = {1477},
   url = {https://doi.org/10.1088/1742-6596/1477/3/032013},
   year = {2020},
}
@book_section{Soares2019,
   author = {José Soares and Patr\'\icia Leite and Paulo Teixeira and Nuno Lopes and Joaquim P Silva},
   journal = {Information Systems for Industry 4.0},
   pages = {1-12},
   publisher = {Springer},
   title = {Data warehouse for the monitoring and analysis of water supply and consumption},
   year = {2019},
}
@inproceedings{Garani2019,
   author = {Georgia Garani and Andrey Chernov and Ilias Savvas and Maria Butakova},
   doi = {10.1109/WETICE.2019.00022},
   journal = {2019 IEEE 28th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)},
   pages = {70-75},
   title = {A Data Warehouse Approach for Business Intelligence},
   year = {2019},
}
@article{Soysal2018,
   abstract = {Existing general clinical natural language processing (NLP) systems such as MetaMap  and Clinical Text Analysis and Knowledge Extraction System have been successfully applied to information extraction from clinical text. However, end users often have to customize existing systems for their individual tasks, which can require substantial NLP skills. Here we present CLAMP (Clinical Language Annotation, Modeling, and Processing), a newly developed clinical NLP toolkit that provides not only state-of-the-art NLP components, but also a user-friendly graphic user interface that can help users quickly build customized NLP pipelines for their individual applications. Our evaluation shows that the CLAMP default pipeline achieved good performance on named entity recognition and concept encoding. We also demonstrate the efficiency of the CLAMP graphic user interface in building customized, high-performance NLP pipelines with 2 use cases, extracting smoking status and lab test values. CLAMP is publicly available for research use, and we believe it is a unique asset for the clinical NLP community.},
   author = {Ergin Soysal and Jingqi Wang and Min Jiang and Yonghui Wu and Serguei Pakhomov and Hongfang Liu and Hua Xu},
   doi = {10.1093/jamia/ocx132},
   issn = {1527-974X (Electronic)},
   issue = {3},
   journal = {Journal of the American Medical Informatics Association : JAMIA},
   month = {3},
   pages = {331-336},
   pmid = {29186491},
   title = {CLAMP - a toolkit for efficiently building customized clinical natural language  processing pipelines.},
   volume = {25},
   year = {2018},
}
@article{Boukhari2018,
   author = {Ilyès Boukhari and Stéphane Jean and Idir Ait-Sadoune and Ladjel Bellatreche},
   doi = {10.1007/s10009-016-0443-0},
   issn = {1433-2787},
   issue = {1},
   journal = {International Journal on Software Tools for Technology Transfer},
   pages = {19-34},
   title = {The role of user requirements in data repository design},
   volume = {20},
   url = {https://doi.org/10.1007/s10009-016-0443-0},
   year = {2018},
}
@inproceedings{Mukherjee2017,
   abstract = {Data Warehouse is a repository of strategic data from many sources gathered over a long period of time. Traditional DW operations mainly comprise of extracting data from multiple sources, transforming these data into a compatible form and finally loading them to DW schema for further analysis. The extract-transform-load (ETL) functions need to be incorporated into appropriate tools so that organisations can utilise these tools efficiently as required. There is a wide variety of such tools available in market. In this paper, we have compared different aspects of some popular ETL tools (Informatica, Datastage, Ab Initio, Oracle Data Integrator, SSIS) and have analysed their advantages and disadvantages. We have also highlighted some salient features (performance optimisation, data lineage, real time data analysis, cost, language binding etc.) of these tools and represented them with a comparative study. Apart from the review of the ETL tools, the paper also provides feedback from data science industry which narrates the market value and relevance of the tools in practical scenario. However, the traditional DW concept is expanding rapidly with the advent of big data, cloud computing, real time data analysis and the growing need of parsing information from structured and unstructured data sources. In this paper, we have also identified these factors which are transforming the definition of data warehousing.},
   author = {R Mukherjee and P Kar},
   doi = {10.1109/IACC.2017.0192},
   isbn = {2473-3571 VO  -},
   journal = {2017 IEEE 7th International Advance Computing Conference (IACC)},
   pages = {943-948},
   title = {A Comparative Review of Data Warehousing ETL Tools with New Trends and Industry Insight},
   year = {2017},
}
@article{Redman2017,
   author = {Joseph S. Redman and Yamini Natarajan and Jason K. Hou and Jingqi Wang and Muzammil Hanif and Hua Feng and Jennifer R. Kramer and Roxanne Desiderio and Hua Xu and Hashem B. El-Serag and Fasiha Kanwal},
   doi = {10.1007/s10620-017-4721-9},
   issue = {10},
   journal = {Digestive diseases and sciences},
   pages = {2713-2718},
   title = {Accurate Identification of Fatty Liver Disease in Data Warehouse Utilizing Natural Language Processing},
   volume = {62},
   year = {2017},
}
@article{Khmelevsky2017a,
   abstract = {ABSTRACT In this presentation we report on two approaches related to engaging and motivating students: programming competitions and applied research projects. We report on student programming competition results of students from the Computer Science Department (COSC) of Okanagan College (OC) and discuss the achieved results from an educational point of view. Students reported that participation in competitions gave them motivation to effectively learn in their programming courses, inspire them to learn deeper and more thoroughly, and help them achieve better results in their classes. In addition we have found that some freshmen and sophomore students in diploma and degree programs are very capable and eager to be involved in applied research projects as early as the second semester. In 2016 COSC won a fifth NSERC Engage College applied research grant with Esri Canada and Gifu-Keizai University (Japan): Roof Damage Assessment from Automated 3D Building (http://tinyurl.com/lz85n7d). The project is still on-going, but students are excited and enjoy working with the GIS systems, tools and 3D modelling. Dr. Sugihara from Gifu-Keizai University conducted training for the students during his short visit to the College and worked with the students. He demonstrated how to digitize building polygons from orthophotography and how to automatically generate general shaped 3D building models with roofs, based on non-orthogonal input building polygons (building footprints). In 2016 COSC students achieved 2nd place in Canada and 125th of 2,200 teams globally at the IEEEXtreme 10th annual student programming competition (http://tinyurl.com/ltk3n87) (compared with 2015, 25th in Canada and in top 500 world-wide http://tinyurl.com/k4utmmg). Then in 2017 a pair of OC computer science students logged an impressive showing at MIT's longest running programming competition âĂIJBattlecode" against 1000+ teams. In Okanagan College's very first appearance at the competition, two freshman students placed 49th overall, competing against top post-secondary institutions from all over the world (http://tinyurl.com/kjxp3mm).},
   author = {Youry Khmelevsky and Ken Chidlow and Kenichi Sugihara and Kongwen Zhang},
   doi = {10.1145/3085585.3088491},
   isbn = {9781450350662},
   journal = {Proceedings of the 22nd Western Canadian Conference on Computing Education},
   month = {5},
   publisher = {ACM},
   title = {Engaging and Motivating Students Through Programming Competitions and GIS Applied Research Projects},
   url = {http://dx.doi.org/10.1145/3085585.3088491},
   year = {2017},
}
@article{Humphrey2017,
   author = {Jack Humphrey and Ty Sutherland},
   title = {The WTFast ’ s Gamers Private Network ( GPN ©},
   year = {2017},
}
@generic{Khan2017,
   abstract = {Byline: Fakhri Alam Khan (a), Awais Ahmad (a), Muhammad Imran (b), Mafawez Alharbi (c), Mujeeb-ur-rehman (a), Bilal Jan (b) * Query performance in Virtual Data Warehouse. * Partitioned materialized views, index performance optimization, etc are used. * The study uses Oracle 10g as a backend database; Oracle management console and SQL query. Article History: Received 14 March 2017; Revised 17 July 2017; Accepted 4 August 2017;Byline: Fakhri Alam Khan [fakhri.alam@imsciences.edu.pk] (a,*), Awais Ahmad (a), Muhammad Imran (b), Mafawez Alharbi (c), Mujeeb-ur-rehman (a), Bilal Jan (b) Keywords Business intelligence; OLTP; Performance optimization; Query optimization; Virtual data warehouse Highlights * Query performance in Virtual Data Warehouse. * Partitioned materialized views, index performance optimization, etc are used. * The study uses Oracle 10 g as a backend database; Oracle management console and SQL query. This paper presents a model for improving query performance in Virtual Data Warehouse (VDW) by simulating VDW environment on a cellular phone billing and customer care system which involves processing millions of Call Detail Records (CDRs) generated by thousands of counters across the country. Processing aggregations on millions of CDRs requires expensive systems, especially when analysing customers' traffic trends and encompasses several performance optimization techniques used for improvement of query performance in VDW. In this regard, VDW offers several advantages such as real-time analytic reports, reduced maintenance, low cost solution and flexible data integration, but performance is still one of its critical shortcomings. This paper enhances performance of VDW by using techniques like partitioned materialized views, index performance optimization, query rewrite in materialized views, analytic functions, sub-queries and enabling parallel execution etc. The study uses Oracle 10 g as a backend database; Oracle management console and SQL query analyser are used for monitoring performance concerns during validation of VDW model; standard PL/SQL developer is used for extracting and loading test data; and finally, Hyperion Development suite is used for testing time comparisons of datasets both in normal OLTP and simulated VDW environments. Author Affiliation: (a) Centre of Excellence in IT, Imsciences Peshawar, Pakistan (b) Sarhad University of Science and Technology, Peshawar, Pakistan (c) Computer Science Department, College of Science, Majmaah University, Saudi Arabia * Corresponding author. Article History: Received 14 March 2017; Revised 17 July 2017; Accepted 4 August 2017;•Query performance in Virtual Data Warehouse.•Partitioned materialized views, index performance optimization, etc are used.•The study uses Oracle 10g as a backend database; Oracle management console and SQL query.  This paper presents a model for improving query performance in Virtual Data Warehouse (VDW) by simulating VDW environment on a cellular phone billing and customer care system which involves processing millions of Call Detail Records (CDRs) generated by thousands of counters across the country. Processing aggregations on millions of CDRs requires expensive systems, especially when analysing customers’ traffic trends and encompasses several performance optimization techniques used for improvement of query performance in VDW. In this regard, VDW offers several advantages such as real-time analytic reports, reduced maintenance, low cost solution and flexible data integration, but performance is still one of its critical shortcomings. This paper enhances performance of VDW by using techniques like partitioned materialized views, index performance optimization, query rewrite in materialized views, analytic functions, sub-queries and enabling parallel execution etc. The study uses Oracle 10g as a backend database; Oracle management console and SQL query analyser are used for monitoring performance concerns during validation of VDW model; standard PL/SQL developer is used for extracting and loading test data; and finally, Hyperion Development suite is used for testing time comparisons of datasets both in normal OLTP and simulated VDW environments.; },
   author = {Fakhri Alam Khan and Awais Ahmad and Muhammad Imran and Mafawez Alharbi and Mujeeb-ur-rehman and Bilal Jan},
   doi = {10.1016/j.scs.2017.08.003},
   isbn = {2210-6707},
   journal = {Sustainable cities and society },
   keywords = {Business intelligence,Data warehousing,Information storage and retrieval,Information storage and retrieval systems,Mathematical optimization,Methods,OLTP,Performance optimization,Query optimization,Usage,Virtual data warehouse},
   pages = {232-240},
   publisher = {Elsevier Ltd },
   title = {Efficient data access and performance improvement model for virtual data warehouse },
   volume = {35 },
   url = {https://go.exlibris.link/jjrcywDr},
   year = {2017},
}
@book_section{Mahboubi2017,
   author = {Hadj Mahboubi and Jé rôme Darmont},
   doi = {10.4018/978-1-61692-016-6.ch014},
   journal = {E-Strategies for Resource Management Systems},
   pages = {232-253},
   publisher = {IGI Global},
   title = {Query Performance Optimization in XML Data Warehouses},
   url = {https://doi.org/10.4018%2F978-1-61692-016-6.ch014},
   year = {2017},
}
@article{Wang2016,
   author = {Xiaoyang; Yang Ping; Liu Hongfang Wang Liwei; Ruan},
   doi = {10.4137/CIN.S40604},
   issn = {1176-9351},
   journal = {Cancer Informatics},
   pages = {CIN.S40604},
   title = {Comparison of Three Information Sources for Smoking Information in Electronic Health Records},
   volume = {15},
   url = {https://browzine.com/articles/156436922},
   year = {2016},
}
@generic{Hassan2015,
   abstract = {Many models have been proposed for modeling multidimensional data warehouses and most consider a same function to determine how measure values are aggregated according to different data detail levels. We provide a conceptual model that supports (1) multiple aggregations, associating to the same measure a different aggregation function according to analysis axes or hierarchies, and (2) differentiated aggregation, allowing specific aggregations at each detail level. Our model is based on a graphical formalism that allows controlling the validity of aggregation functions (distributive, algebraic or holistic). We also show how conceptual modeling can be used, in an R-OLAP environment, for building lattices of pre-computed aggregates.},
   author = {Ali Hassan and Frank Ravat and Olivier Teste and Ronan Tournier and Gilles Zurfluh},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-662-47804-2_2},
   isbn = {0302-9743},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Aggregate lattice,Aggregation functions,Conceptual modeling,Data warehouse,Multiple aggregations},
   pages = {20-47},
   publisher = {Springer Berlin Heidelberg},
   title = {Differentiated multiple aggregations in multidimensional databases},
   volume = {9260},
   url = {https://go.exlibris.link/ZTRhh7Qr},
   year = {2015},
}
@article{Wang2015,
   abstract = {The rapidly increasing scale of data warehouses is challenging today’s data analytical technologies. A conventional data analytical platform processes data warehouse queries using a star schema — it normalizes the data into a fact table and a number of dimension tables, and during query processing it selectively joins the tables according to users’ demands. This model is space economical. However, it faces two problems when applied to big data. First, join is an expensive operation, which prohibits a parallel database or a MapReduce-based system from achieving efficiency and scalability simultaneously. Second, join operations have to be executed repeatedly, while numerous join results can actually be reused by different queries. In this paper, we propose a new query processing framework for data warehouses. It pushes the join operations partially to the pre-processing phase and partially to the post-processing phase, so that data warehouse queries can be transformed into massive parallelized filter-aggregation operations on the fact table. In contrast to the conventional query processing models, our approach is efficient, scalable and stable despite of the large number of tables involved in the join. It is especially suitable for a large-scale parallel data warehouse. Our empirical evaluation on Hadoop shows that our framework exhibits linear scalability and outperforms some existing approaches by an order of magnitude.},
   author = {Huiju Wang and Xiongpai Qin and Xuan Zhou and Furong Li and Zuoyan Qin and Qing Zhu and Shan Wang},
   doi = {10.1007/s11704-014-4025-6},
   issn = {20952236},
   issue = {2},
   journal = {Frontiers of Computer Science},
   keywords = {TAMP,data warehouse,join-free,large scale,multi-version schema},
   pages = {224-236},
   title = {Efficient query processing framework for big data warehouse: an almost join-free approach},
   volume = {9},
   url = {https://doi.org/10.1007/s11704-014-4025-6},
   year = {2015},
}
@article{Harbor2014,
   author = {National Harbor},
   issue = {July},
   title = {CIM-based Utility Data Model Solution for Enterprise Analytics},
   year = {2014},
}
@article{Dhomne2013,
   author = {Pooja A Dhomne and Sheetal R Gajbhiye and Tejaswini S Warambhe and V Bhagat},
   journal = {International Journal of Research in Engineering and Technology},
   pages = {589-594},
   title = {ACCESSING DATABASE USING NLP},
   volume = {02},
   year = {2013},
}
@book{Kimball2013,
   author = {Ralph Kimball and Margy Ross},
   edition = {3},
   publisher = {O'Reilly},
   title = {The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling},
   year = {2013},
}
@inproceedings{Grmek2011,
   abstract = {In this paper we investigate possible solutions for alleviating retail manufacturers of logistical concerns by using inexpensive cell phones with WAP and WiFi capabilities, low resolution digital cameras, and open source applications for web hosts in the cloud to store and process business information. The proposed inventory tracking system prototype is aimed at the company's agents whose responsibilities are to track and manage the retailer's merchandise as it flows between suppliers and consumers. The system can eliminate inefficiencies in the process of tracking inventory and orders processing, while doing so with minimal economic cost by utilizing inexpensive cell phones from one side and inexpensive web hosting in the cloud on the other side. This means to use inexpensive options in terms of both hardware and software, and services in the cloud for data processing and storage as well as to automate the process of physically tracking inventory so less time is spent on this particular task. Such a system with further development can also address business critical question of monitoring sales personnel adherence to the assigned sales routes, collection of other information from the retail outlets (products distribution, pricing, shelving, out-of-stock situations etc.). There are several areas where the proposed solution can be used: on-shelf availability check and inventory calculation (used both by retailers' personnel and by the manufacturer's sales force); orders taking (to automate the process); retail audit (used by the specialized commercial or governmental agencies) and by consumer protection rights agencies. From the technical point of view the goal is to investigate the available open source solutions so they may be integrated with a new proposed system for business utilization. The paper outlines the design of the proposed system and the prototype implementation results, as well as our problems during prototype design and development, and our future plans.},
   author = {Rob Grmek and Youry Khmelevsky and Dmitry Syrotovsky and Grmek R and Khmelevsky Y and Syrotovsky D},
   doi = {10.1109/HPCSim.2011.5999857},
   isbn = {9781612843810},
   journal = {Proceedings of the 2011 International Conference on High Performance Computing and Simulation, HPCS 2011},
   keywords = {Cell Phone,Cloud,Inventory Tracking System,Mobile Cloud Computing,OCR,Open Source,Web Hosting},
   pages = {435-441},
   title = {Automated inventory tracking system prototype in cloud},
   year = {2011},
}
@inproceedings{Perez2011,
   abstract = {The 2011 ITiCSE working group on information assurance (IA) education examined undergraduate curricula at the two- and four year levels, both within and outside the United States (US). A broad set of two-year IA degree programs were examined in order to get a sense of similarities and differences between them. A broad set of four-year IA degree programs were also examined to explore their similarities and differences. A comparison between the two-year and fourfour-year degree programs revealed that the common challenge of articulation between two- and four-year programs exists in IA as well. The challenge of articulation was explored in some depth in order to understand what remedies might be available. Finally, a number of IA programs at international institutions were examined in order to gain insight into differences between US and non-US IA programs.},
   author = {Lance C. Pérez and Margaret Leary and Amelia Philips and Norbert Pohlmann and Blair Taylor and Shambhu Upadhyaya and Stephen Cooper and Elizabeth K. Hawthorne and Susanne Wetzel and Joel Brynielsson and Asim Gencer Gökce and John Impagliazzo and Youry Khmelevsky and Karl Klee},
   doi = {10.1145/2078856.2078860},
   isbn = {9781450311229},
   issn = {1942647X},
   journal = {Proceedings of the 16th annual conference reports on Innovation and technology in computer science education - working group reports - ITiCSE-WGR '11},
   title = {Information assurance education in two- and four-year institutions},
   year = {2011},
}
@inproceedings{Khmelevsky2011a,
   author = {Youry Khmelevsky},
   city = {New York, NY, USA},
   doi = {10.1145/1989622.1989638},
   isbn = {978-1-4503-0792-5},
   journal = {Proceedings of the 16th Western Canadian Conference on Computing Education},
   keywords = {research career,research experiences for undergraduates,software engineering,student projects,teaching strategy,undergraduate research},
   pages = {57-60},
   publisher = {ACM},
   title = {Research and Teaching Strategies Integration at Post-secondary Programs},
   url = {http://doi.acm.org/10.1145/1989622.1989638},
   year = {2011},
}
@article{Reddy2010,
   author = {G Satyanarayana Reddy and Rallabandi Srinivasu and M Poorna Chander Rao and Srikanth Reddy Rikkula},
   issue = {9},
   journal = {International Journal on Computer Science and Engineering},
   pages = {2865-2873},
   publisher = {Citeseer},
   title = {Data Warehousing, Data Mining, OLAP and OLTP Technologies are essential elements to support decision-making process in industries},
   volume = {2},
   year = {2010},
}
@article{Savova2010,
   abstract = {We aim to build and evaluate an open-source natural language processing system for  information extraction from electronic medical record clinical free-text. We describe and evaluate our system, the clinical Text Analysis and Knowledge Extraction System (cTAKES), released open-source at http://www.ohnlp.org. The cTAKES builds on existing open-source technologies-the Unstructured Information Management Architecture framework and OpenNLP natural language processing toolkit. Its components, specifically trained for the clinical domain, create rich linguistic and semantic annotations. Performance of individual components: sentence boundary detector accuracy=0.949; tokenizer accuracy=0.949; part-of-speech tagger accuracy=0.936; shallow parser F-score=0.924; named entity recognizer and system-level evaluation F-score=0.715 for exact and 0.824 for overlapping spans, and accuracy for concept mapping, negation, and status attributes for exact and overlapping spans of 0.957, 0.943, 0.859, and 0.580, 0.939, and 0.839, respectively. Overall performance is discussed against five applications. The cTAKES annotations are the foundation for methods and modules for higher-level semantic processing of clinical free-text.},
   author = {Guergana K Savova and James J Masanz and Philip V Ogren and Jiaping Zheng and Sunghwan Sohn and Karin C Kipper-Schuler and Christopher G Chute},
   doi = {10.1136/jamia.2009.001560},
   issn = {1527-974X (Electronic)},
   issue = {5},
   journal = {Journal of the American Medical Informatics Association : JAMIA},
   keywords = {Biomedical Research,Electronic Health Records,Information Storage and Retrieval,Natural Language Processing,methods},
   pages = {507-513},
   pmid = {20819853},
   title = {Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture,  component evaluation and applications.},
   volume = {17},
   year = {2010},
}
@inproceedings{Khmelevsky2009a,
   abstract = {This article summarizes collaborative educational activities and joint educational program development between the Computer Science department and other departments at the same and other institutions. We hope that this article will aid Universities, ...},
   author = {Youry Khmelevsky and Michael Govorov and Leif Burge},
   doi = {10.1145/1536274.1536293},
   isbn = {9781605584157},
   issn = {2079-9721 (Electronic)},
   journal = {Proceedings of the 14th Western Canadian Conference on Computing Education - WCCCE '09},
   keywords = {arcmap,arcsde,computer science education,esri,geographical information systems,oracle academic initiative (oai),projects},
   pages = {65-69},
   pmid = {28933354},
   title = {Okanagan College and Vancouver Island University educational joint projects results},
   url = {http://portal.acm.org/citation.cfm?id=1536274.1536293},
   year = {2009},
}
@inproceedings{Zaker2008,
   abstract = {Building indexes on database is common, but it has an important impact on the query performance, especially in large databases such as a Data Warehouse where the queries are usually very complex and ad hoc. If a proper index structure is chosen, the query response time can be accelerated. Until now, there is no definite guideline for Data Warehouse analysts to choose the appropriate index. According to conventional wisdom, Bitmap index is a preferred indexing technique for cases where the indexed attributes have few distinct values (i.e., low cardinality). The query response time is expected to degrade as the cardinality of indexed columns increase due to a larger index size. On the other hand, B-tree index is good if the column values are of high cardinality due to its indexing and retrieving mechanisms. In this paper, we show that this may not be true under certain circumstances. Experimental results support the fact that even though the level of column cardinality determines the index file size, but the query processing time is not determined by the level of column cardinality. Moreover, our results indicate that the Bitmap index is faster than B-tree index on a large dataset with multi-billion records.},
   author = {Morteza Zaker and Somnuk Phon-Amnuaisuk and Su-Cheng Haw},
   city = {Stevens Point, Wisconsin, USA},
   isbn = {9789604740284},
   journal = {Proceedings of the 8th Conference on Applied Computer Scince},
   keywords = {B-tree index,bitmap index,data warehouse,query processing},
   pages = {123–130},
   publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
   title = {Investigating Design Choices between Bitmap Index and B-Tree Index for a Large Data Warehouse System},
   year = {2008},
}
@generic{Rosenberg2006,
   abstract = {Modern data warehouses, especially those deployed for the "intelligent enterprise," are characterized by rapid increases in data volume and the number of users. Apart from data-quality concerns, the sheer volume of the data poses a real challenge, and this problem will only increase as more data sources are pulled into the business intelligence (BI) orbit. The current trend to widely deploy BI tools in the organization means more users will compete for the same-already strained-computing resources. Analysis complexity increases as business users find insights by examining data from many angles and relating it in different ways. Furthermore, whereas early data warehouses' primary usage mode was large and predictable batch reports, today the "real-time enterprise" implies interactive analysis that requires fast query-response times. On a technical level, these factors translate into extremely complex and concurrent queries over large data sets, pushing the limits of existing databases. As a result, more organizations are facing acute query-performance problems. If left unresolved, these problems will diminish the business value of deploying larger data warehouses and giving analytical tools to more users. This article analyzes available technologies and methods for tackling such query-performance issues. We discuss six different approaches. Some may be traditional and familiar, while others are less obvious and bring innovation into the solution space. We evaluate their pros and cons and provide data warehouse practitioners with new perspectives to facilitate informed decision making. [PUBLICATION ABSTRACT] },
   author = {Alex Rosenberg},
   city = {Seattle },
   isbn = {1547-2825},
   issue = {1 },
   journal = {Business intelligence journal },
   keywords = {Business intelligence software,Computer centers,Costs,Data warehouses,Databases,Licenses,Queries,Servers,Studies},
   pages = {7},
   publisher = {Data Warehousing Institute },
   title = {Improving Query Performance in Data Warehouses },
   volume = {11 },
   url = {https://go.exlibris.link/m4kHc20P},
   year = {2006},
}
@article{Chan2011,
   author = {SkillSoft Corporation},
   isbn = {9781849682602},
   issue = {2},
   title = {Oracle Database Performance Tuning},
   volume = {12},
   url = {https://docs.oracle.com/cd/E28271_01/server.1111/e16638.pdf},
   year = {2003},
}
@generic{Martens2003,
   abstract = {Parallel processing is a key to high performance in very large data warehouse applications that execute complex analytical queries on huge amounts of data. Although parallel database systems (PDBSs) have been studied extensively in the past decades, the specifics of load balancing in parallel data warehouses have not been addressed in detail. In this study, we investigate how the load balancing potential of a Shared Disk (SD) architecture can be utilized for data warehouse applications. We propose an integrated scheduling strategy that simultaneously considers both processors and disks, regarding not only the total workload on each resource but also the distribution of load over time. We evaluate the performance of the new method in a comprehensive simulation study and compare it to several other approaches. The analysis incorporates skew aspects and considers typical data warehouse features such as star schemas with large fact tables and bitmap indices.;Parallel processing is a key to high performance in very large data warehouse applications that execute complex analytical queries on huge amounts of data. Although parallel database systems (PDBSs) have been studied extensively in the past decades, the specifics of load balancing in parallel data warehouses have not been addressed in detail.  In this study, we investigate how the load balancing potential of a Shared Disk (SD) architecture can be utilized for data warehouse applications. We propose an integrated scheduling strategy that simultaneously considers both processors and disks, regarding not only the total workload on each resource but also the distribution of load over time. We evaluate the performance of the new method in a comprehensive simulation study and compare it to several other approaches. The analysis incorporates skew aspects and considers typical data warehouse features such as star schemas with large fact tables and bitmap indices. Copyright © 2003 John Wiley & Sons, Ltd.; },
   author = {Holger Märtens and Erhard Rahm and Thomas Stöhr},
   city = {Chichester, UK },
   doi = {10.1002/cpe.786},
   isbn = {1096-9128;1040-3108;1532-0626;},
   issue = {15;11-12; },
   journal = {Concurrency and Computation: Practice and Experience },
   keywords = {Shared Disk architecture,data warehousing,disk contention,load balancing,parallel database systems,query optimization},
   pages = {1169-1190},
   publisher = {John Wiley & Sons, Ltd },
   title = {Dynamic query scheduling in parallel data warehouses },
   volume = {15;12; },
   url = {http://ubc.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwjV3da9swED-25GUvS7utNOvH9FD25tSRY8t-LGnTPWVh9AP6IiT53I4FJzgLbf_73ll2aAuFgsFg2Vj3Id1JuvsdQCQHYfBqTkjCwrokM4m1KY5CzLMiKbJEkb1BZ-zrUB3VpsZ4uIjN_hsPlHr65vFu7Ko55pfHbokDlSYfoTscqoj0vXs6vvl9vjlP4GIGHjl},
   year = {2003},
}
@generic{Pedersen2001,
   abstract = {Data warehouses are becoming increasingly popular in the spatial domain, where they are used to analyze large amounts of spatial information for decision-making purposes. The data warehouse must provide very fast response times if popular analysis tools such as On-Line Analytical Processing [2](OLAP) are to be applied successfully. In order for the data analysis to have an adequate performance, pre-aggregation, i.e., pre-computation of partial query answers, is used to speed up query processing. Normally, the data structures in the data warehouse have to be very “well-behaved” in order for pre-aggregation to be feasible. However, this is not the case in many spatial applications. In this paper, we analyze the properties of topological relationships between 2D spatial objects with respect to pre-aggregation and show why traditional preaggregation techniques do not work in this setting. We then use this knowledge to significantly extend previous work on pre-aggregation for irregular data structures to also cover special spatial issues such as partially overlapping areas.},
   author = {Torben Bach Pedersen and Nektaria Tryfona},
   city = {Berlin, Heidelberg},
   doi = {10.1007/3-540-47724-1_24},
   isbn = {9783540423010},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Aggregation Function,Applied sciences,Bare Ground,Computer science,Exact sciences and technology,Information systems. Data bases,Land Cover,Memory organisation. Data processing,Software,Spatial Object,Topological Relationship,control theory,systems},
   pages = {460-478},
   publisher = {Springer Berlin Heidelberg},
   title = {Pre-aggregation in spatial data warehouses},
   volume = {2121},
   url = {https://go.exlibris.link/d7YZB7GP},
   year = {2001},
}
@inproceedings{Sutton1999,
   abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
   author = {Richard S Sutton and David McAllester and Satinder Singh and Yishay Mansour},
   city = {Cambridge, MA, USA},
   journal = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
   pages = {1057–1063},
   publisher = {MIT Press},
   title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
   year = {1999},
}
@thesis{Khmelevsky1992,
   author = {Youry Khmelevsky},
   city = {Kyiv Scientific Typography at Scientific Book Publisher, Kyiv, Ukraine},
   institution = {The Institute of Simulation Problems in Power Industry},
   keywords = {Parameterization,Statistical Analyses,Stochastic Signals},
   month = {9},
   title = {Parameterization and Statistical Analysis of Stochastic Signals in Biological Research},
   year = {1992},
}
@inproceedings{Hains2020,
   author = {Gaétan Hains and Chris Mazur and Jesse Ayers and Jack Humphrey and Youry Khmelevsky and Ty Sutherland},
   institution = {IEEE},
   journal = {2020 IEEE International Systems Conference (SysCon)},
   pages = {1-6},
   title = {The WTFast’s Gamers Private Network (GPN®) Performance Evaluation Results},
}
@generic{Toumi2014,
   author = {Lyazid Toumi and Abdelouahab Moussaoui and Ahmet Ugur},
   city = {Boston},
   doi = {10.1007/s11227-013-1058-9},
   isbn = {0920-8542},
   issue = {2},
   journal = {The Journal of supercomputing},
   keywords = {Algorithms,Analysis,Article,Compilers,Computer Science,Data mining,Data warehousing,Databases,Interpreters,Mathematical models,Mathematical optimization,Optimization,Performance indices,Processor Architectures,Programming Languages,Queries,Stars,Swarm intelligence,general},
   pages = {672-708},
   publisher = {Springer US},
   title = {Particle swarm optimization for bitmap join indexes selection problem in data warehouses},
   volume = {68},
   url = {https://go.exlibris.link/7pgBfHNs},
}
